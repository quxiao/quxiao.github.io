---
layout: post
title: "《Infomration Retrieval in Practice》笔记——整体架构"
description: ""
category: ""
tags: []
---
{% include JB/setup %}


对于一个典型的搜索引擎，至少包含两大部分：1）建立索引；2）query查询。

建立索引
=======================

主要流程如下图所示：

![pic]({{site.url}}/images/2013-11-30/index-building.png)

就是获取需要检索的数据，将其转化为系统可以识别的方式，并且做一些预处理。

本文获取
---------------

** 爬虫 **

本文获取就是将所需要的数据“抓”下来。其中，依据搜索引擎的使用场景不同(网站内部检索/全网检索/文件系统检索/...)，需要检索的数据可以是全网、某个网站的网页、数据库的表、或者磁盘上面的文件等等，这样就需要不同类型的“爬虫”。另外，也可以像`RSS`这样的流式访问数据的标准

获取到所需要的数据，这只完成了一部分。另外，当数据源有更新时（网页修改/数据库新增了数据/...），爬虫需要能及时的感知并且重新抓取数据提供给检索系统。而且，不同类型的数据，对于实时性的要求是不一样的。比如，一个发布新闻网站和和一个保存历史资料的网站，它们网页的实时性要求肯定是不一样的。

** 转换 **

文本获取到了，需要将其统一成搜索引擎可以识别的格式，文本转换包含两层含义：

1. **将不同格式标准的数据转换为统一的格式**。例如，同样的记录书籍内容的文档，一个是`XML`格式的，一个是`JSON`格式的。爬虫如果想获取书籍的名称、作者、`ISBN`等信息，就需要采用不同的解析方式。
2. **将不同编码的文本转化为统一编码**。有的网页是`UTF-8`编码的，有的是`GB18030`编码的，都需要统一成一个编码。

** 保存 **

数据获取到了，也转成统一格式了，下面就得想办法把数据持久化。持久化的方式有很多种，比如可以是本地文件系统、分布式文件系统（HDFS）、各种数据库（MYSQL / MongoDB）等等。



